<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>abstraction-vs-concreteness</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="the-art-of-appropriate-abstraction-in-programming">The Art of
Appropriate Abstraction in Programming</h1>
<h2 id="abstract">Abstract</h2>
<p>This paper examines how software engineers should calibrate
abstraction levels in their designs. Drawing on formal foundations
(λ-calculus, category theory, abstract interpretation), historical
perspectives from pioneers including Dijkstra, Hoare, Perlis, Liskov,
and Simon, and empirical research on type systems, I argue that neither
“always abstract” nor “always concrete” positions are defensible.
Instead, I propose that <strong>the appropriate level of abstraction
should match the commitment level of decisions</strong>: abstract where
changes are costly and irreversible (public APIs, protocols,
safety-critical systems), stay concrete where exploration is cheap and
decisions are reversible (internal code, prototypes). This framework
reconciles the formal methods tradition of stepwise refinement with
practical software engineering wisdom, providing actionable guidance for
working engineers.</p>
<p><strong>Keywords:</strong> abstraction, software design, type
systems, formal methods, cognitive load, programming languages</p>
<hr />
<h2 id="introduction-the-challenge-of-calibration">1. Introduction: The
Challenge of Calibration</h2>
<blockquote>
<p>“Functions delay binding; data structures induce binding. Moral:
Structure data late in the programming process.” — Alan Perlis, Epigrams
on Programming (1982)</p>
</blockquote>
<p>This single epigram captures one of the most fundamental trade-offs
in software design. When we write a function, we defer commitment. The
caller decides what data flows through it, when to invoke it, how to
compose it with other functions. When we define a data structure, we
crystallize assumptions. The fields we choose, the types we assign, the
relationships we encode: all become constraints that propagate through
every piece of code that touches that structure.</p>
<p><strong>A Running Example: The Order Processing System</strong></p>
<p>Consider a team building an e-commerce order processing system. They
face abstraction decisions at every turn. Should <code>Order</code> be a
concrete class with fixed fields, or an interface that different order
types implement? Should payment processing be abstracted behind a
<code>PaymentProvider</code> interface from day one, or should they
start with a concrete Stripe implementation? Should the pricing logic
use a generic <code>PricingRule</code> abstraction, or hard-code the
current discount rules?</p>
<p>These are not academic questions. The wrong abstraction too early
creates interfaces that do not match real requirements. The wrong
concreteness too early creates brittle code that cannot accommodate the
second payment provider or the promotional pricing that marketing will
inevitably request. We will return to this example throughout the paper
to ground formal concepts in practical decisions.</p>
<p>Effective programming requires calibrating abstraction to context. On
one side lies abstraction: the power to defer decisions, preserve
flexibility, and reason about systems at higher levels without drowning
in detail. On the other side lies concreteness: the clarity of explicit
representations, the safety of compile-time checks, and the immediate
comprehensibility of code that shows rather than tells.</p>
<p>Neither extreme serves us well. Pure abstraction produces systems
that are flexible in theory but incomprehensible in practice, code that
could do anything but communicates nothing about what it actually does.
Pure concreteness produces brittle systems, easy to read line-by-line
but impossible to change without cascading modifications.</p>
<p><strong>This paper’s thesis:</strong> The appropriate level of
abstraction should match the commitment level of decisions. Abstract
where changes are costly and irreversible—public APIs, protocols,
safety-critical systems. Stay concrete where exploration is cheap and
decisions are reversible—internal code, prototypes, early-stage
development. This principle, developed fully in Section 8, provides a
practical framework for navigating the abstraction decision.</p>
<figure>
<img src="abstraction-commitment-framework.svg"
alt="The Abstraction-Commitment Framework" />
<figcaption aria-hidden="true">The Abstraction-Commitment
Framework</figcaption>
</figure>
<p><strong>Figure 1.</strong> The abstraction-commitment framework. The
diagonal represents appropriate calibration: low-commitment decisions
(internal code, prototypes) warrant concrete implementations;
high-commitment decisions (public APIs, protocols) warrant abstract
interfaces. The shaded regions indicate common failure modes.</p>
<p>The giants of our field have grappled with this challenge from the
beginning. Dijkstra sought abstraction as a tool for precision, not
vagueness. Hoare observed that we can make systems simple enough to be
obviously correct or complicated enough to have no obvious deficiencies.
Perlis counseled us to structure data late, to keep options open until
we know enough to commit wisely. Herbert Simon explained why complex
systems require hierarchical abstraction to evolve and be understood.
Church’s λ-calculus formalized abstraction as the foundational operation
of computation itself.</p>
<p>This paper explores the art of appropriate abstraction across
multiple dimensions: the formal foundations of abstraction, the
historical perspectives of programming’s pioneers, the trade-offs in
type system design, case studies from modern languages, the cognitive
realities of how programmers reason about code, and empirical evidence
on what actually works. While I survey multiple perspectives, I argue
for a practical principle: the appropriate level of abstraction is
determined by the cost and reversibility of decisions. Abstract where
commitments are expensive; stay concrete where exploration is cheap.</p>
<hr />
<h2 id="formal-foundations-of-abstraction">2. Formal Foundations of
Abstraction</h2>
<p>Before examining practical trade-offs, we must understand what
abstraction <em>is</em> at a fundamental level. Three formal frameworks
illuminate different aspects of abstraction in computation. (Readers
primarily interested in practical guidance may skim this section and
proceed to Section 3, returning here for theoretical grounding as
needed.)</p>
<h3 id="λ-calculus-abstraction-as-variable-binding">2.1 λ-Calculus:
Abstraction as Variable Binding</h3>
<p>Alonzo Church’s λ-calculus (1936) established
<strong>λ-abstraction</strong> as the primitive operation for defining
functions. The expression <code>λx.M</code> represents a function that,
given an argument <code>x</code>, returns the term <code>M</code>. This
is the mechanism by which we abstract over a variable, creating a
general pattern that can be instantiated with any value.</p>
<p>In λ-calculus, abstraction means generalizing an expression by
replacing a specific value with a variable, then binding that
variable:</p>
<ul>
<li>Concrete: <code>3 + 5</code> (specific computation)</li>
<li>Abstract: <code>λx.x + 5</code> (pattern—“add 5 to something”)</li>
<li>More abstract: <code>λx.λy.x + y</code> (pattern—“add two
things”)</li>
</ul>
<p>Church and Turing independently established that λ-calculus has
equivalent computational power to Turing machines, meaning all
computation can be expressed through abstraction and application alone.
Every function definition in programming is a form of λ-abstraction.
When we describe code as “too concrete” or “insufficiently abstract,” we
are observing that it has not used λ-abstraction to factor out the
varying parts.</p>
<p>In our order processing example, consider pricing calculation. A
concrete implementation might be
<code>calculatePrice(item, quantity) = item.basePrice * quantity * 0.9</code>
(hard-coding a 10% discount). The abstraction
<code>λdiscount. λitem. λquantity. item.basePrice * quantity * discount</code>
factors out the discount rate, allowing different promotions without
code changes. The question is not whether to abstract, but when: before
we understand the discount patterns, or after we have seen enough
variation to know what varies.</p>
<h3 id="category-theory-abstraction-as-structure-preservation">2.2
Category Theory: Abstraction as Structure Preservation</h3>
<p>Category theory provides a mathematical framework for abstraction
itself—the study of structure-preserving mappings between mathematical
structures. In programming, this manifests through functors, natural
transformations, and monads.</p>
<p>Eugenio Moggi’s paper “Notions of computation and monads” (1991)
established monads as the categorical foundation for computational
effects. A monad separates “what we compute” from “how we compute it.”
The type constructor <code>T</code> lifts pure values into a
computational context; the operations <code>unit</code> and
<code>bind</code> define how computations compose.</p>
<p>Philip Wadler popularized this in “Monads for functional programming”
(1995):</p>
<blockquote>
<p>“Monads provide a way to structure programs generically.”</p>
</blockquote>
<p>Wadler often characterized monads as a kind of “programmable
semicolon”—they define how computations are sequenced.</p>
<p>Monads abstract over computational patterns: <code>Maybe</code>
abstracts possible failure, <code>Either</code> abstracts error
handling, <code>IO</code> abstracts side effects, <code>State</code>
abstracts mutable state. Abstraction here is not merely hiding
implementation details. It is capturing computational patterns as
first-class entities with laws that guarantee behavior.</p>
<h3
id="abstract-interpretation-abstraction-as-controlled-information-loss">2.3
Abstract Interpretation: Abstraction as Controlled Information Loss</h3>
<p>Abstract interpretation, introduced by Patrick and Radhia Cousot
(1977), provides a formal framework for understanding what abstraction
is and what it preserves. The central concept is the Galois connection
between concrete and abstract domains.</p>
<p>In the standard formulation, the concrete domain C is a complete
lattice (often a powerset representing sets of possible program states),
and the abstract domain A is a simpler lattice. The connection consists
of:</p>
<ul>
<li><strong>α: C → A</strong> (abstraction function): maps concrete
elements to abstract elements</li>
<li><strong>γ: A → C</strong> (concretization function): maps abstract
elements back to concrete elements</li>
</ul>
<p>The key property is that α and γ form an adjunction: α(c) ⊑ a if and
only if c ≤ γ(a). Intuitively, abstraction loses information (many
concrete values may map to the same abstract value), while
concretization recovers all possibilities consistent with the abstract
value.</p>
<p>The framework formalizes how abstraction fundamentally involves
deliberately losing information, but losing it in a controlled,
principled way. An abstract interpretation is <em>sound</em> if the
abstraction never lies about the concrete; it may sacrifice
<em>completeness</em> by not telling the whole truth.</p>
<p>Many type systems can be understood as abstract interpretations. A
type like <code>Int</code> abstracts over all possible integer values,
losing precision but gaining static guarantees. There is always a
precision-tractability trade-off: more abstract means more general but
less precise.</p>
<hr />
<h2 id="historical-foundations-the-pioneers-views">3. Historical
Foundations: The Pioneers’ Views</h2>
<h3 id="alan-perlis-and-the-wisdom-of-deferred-commitment">3.1 Alan
Perlis and the Wisdom of Deferred Commitment</h3>
<p>Alan Perlis, the first recipient of the Turing Award, wrote his
influential “Epigrams on Programming” in 1982, distilling decades of
experience into concise observations that remain relevant today. His
epigram—“Functions delay binding; data structures induce
binding”—emerged from the Lisp tradition where late binding was not just
possible but idiomatic.</p>
<p>It is important to understand what Perlis meant by “binding” here. He
was not primarily discussing runtime polymorphism or object-oriented
late binding. Rather, he was making a point about <strong>design
timing</strong>—when in the development process you commit to specific
representations. Functions provide flexibility because they do not
commit to data representation; data structures force early commitment to
how information is organized.</p>
<p>Perlis’s ninth epigram extends this insight: “It is better to have
100 functions operate on one data structure than 10 functions on 10 data
structures.” The point is about <strong>composability</strong>: many
functions sharing a common data format can be combined freely, with no
conversion needed between subsystems.</p>
<p>His thirty-first epigram offers a crucial caveat: “Simplicity does
not precede complexity, but follows it.” The simple abstractions we
admire are not naive starting points but carefully earned insights. They
emerge from understanding complexity deeply enough to see through
it.</p>
<p><strong>Perlis also warned about the costs of abstraction.</strong>
His twentieth epigram cautions: “Wherever there is modularity there is
the potential for misunderstanding: Hiding information implies a need to
check communication.” His fifty-fourth warns against excessive
generality: “Beware of the Turing tar-pit in which everything is
possible but nothing of interest is easy.” And his fifty-eighth
establishes a hierarchy: “Fools ignore complexity. Pragmatists suffer
it. Some can avoid it. Geniuses remove it.” Note that the genius’s move
is <em>removal</em>, not hiding.</p>
<h3 id="dijkstra-on-abstraction-as-precision">3.2 Dijkstra on
Abstraction as Precision</h3>
<p>Edsger Dijkstra’s conception of abstraction differs subtly but
importantly from the Lisp tradition. For Dijkstra, abstraction was not
primarily about flexibility but about precision:</p>
<blockquote>
<p>“The purpose of abstracting is not to be vague, but to create a new
semantic level in which one can be absolutely precise.” — “The Humble
Programmer,” ACM Turing Award Lecture (1972)</p>
</blockquote>
<p>This conception treats abstraction as stratified layers. Each layer
has its own vocabulary, its own rules, its own proofs. The network stack
abstracts physical signals into packets, packets into connections,
connections into application protocols. Each level is precise within its
own terms—we can prove properties about TCP without reasoning about
electrical voltages.</p>
<p>Dijkstra’s famous advocacy for separation of concerns follows from
this view. In “On the Role of Scientific Thought” (EWD 447, 1974), he
described separation of concerns as “the only available technique for
effective ordering of one’s thoughts,” while acknowledging it is “not
perfectly possible.” The full context reveals that Dijkstra saw this
primarily as a <strong>thinking technique</strong>, not an architectural
prescription. He advocated studying aspects of a problem in
isolation—correctness on one day, efficiency on another—to manage
cognitive load during analysis. The separation exists in our reasoning,
even when the concerns remain coupled in the code.</p>
<p>Notably, Dijkstra was skeptical of certain kinds of abstraction. He
famously criticized object-oriented programming and was concerned about
abstractions that enabled “programming by trial and error” rather than
formal reasoning. He favored abstractions that enabled mathematical
proof, not merely flexibility.</p>
<h3 id="tony-hoares-two-ways">3.3 Tony Hoare’s Two Ways</h3>
<p>Tony Hoare crystallized the central dilemma in his 1980 ACM Turing
Award lecture (published 1981), “The Emperor’s Old Clothes”:</p>
<blockquote>
<p>“There are two ways of constructing a software design: One way is to
make it so simple that there are obviously no deficiencies, and the
other way is to make it so complicated that there are no obvious
deficiencies. The first method is far more difficult.”</p>
</blockquote>
<p>The first path, obviously correct systems, requires abstraction. We
must suppress enough detail that the essential logic becomes visible and
verifiable. The second path is the default outcome when we fail at
abstraction: complexity accumulates through accretion until we can no
longer see whether the system is correct.</p>
<p>Hoare pioneered abstract program verification (Hoare Logic), showing
he valued abstraction that enabled formal reasoning, not abstraction for
its own sake.</p>
<h3 id="barbara-liskov-and-information-hiding">3.4 Barbara Liskov and
Information Hiding</h3>
<p>Barbara Liskov’s work on abstract data types (1974), later embodied
in the CLU language (1975), provided a mechanism for Dijkstra’s vision.
An abstract data type hides its representation behind an interface.
Clients push and pop stacks, enqueue and dequeue queues, without knowing
whether the implementation uses arrays or linked lists.</p>
<p>The Liskov Substitution Principle, formalized with Jeannette Wing in
1994, specifies when abstraction succeeds: a subtype must be
substitutable for its parent type without altering the correctness of
the program. The formal definition involves behavioral constraints:</p>
<ul>
<li><strong>Precondition weakening</strong>: The subtype may accept more
inputs</li>
<li><strong>Postcondition strengthening</strong>: The subtype may
provide stronger guarantees</li>
<li><strong>Invariant preservation</strong>: The subtype must maintain
the supertype’s invariants</li>
</ul>
<p>Liskov emphasized that abstractions must be precisely specified;
vague or incomplete abstractions are harmful.</p>
<h3 id="herbert-simon-and-the-architecture-of-complexity">3.5 Herbert
Simon and the Architecture of Complexity</h3>
<p>Herbert Simon’s “The Sciences of the Artificial” (1969; 3rd ed. 1996)
provides theoretical grounding for why abstraction is necessary in
complex systems. Simon introduced the concept of “nearly decomposable
systems”:</p>
<blockquote>
<p>“In a nearly decomposable system, the short-run behavior of each of
the component subsystems is approximately independent of the short-run
behavior of the other components.”</p>
</blockquote>
<p>His watchmaker parable contrasts two approaches to building complex
artifacts. Tempus builds watches as monolithic assemblies; if
interrupted, he must start over. Hora builds watches from stable
subassemblies; interruptions only lose the current subassembly. Simon
concludes:</p>
<blockquote>
<p>“Complex systems will evolve from simple systems much more rapidly if
there are stable intermediate forms than if there are not.”</p>
</blockquote>
<p>This explains why we abstract: not just for understanding, but for
evolutionary stability. Good abstractions are “stable intermediate
forms” that allow systems to evolve without catastrophic rewriting.</p>
<p>In our order processing system, the <code>Order</code> aggregate is a
stable intermediate form. Whether the team later adds subscription
orders, gift orders, or pre-orders, the core <code>Order</code>
abstraction can remain stable while implementations vary. If they had
built order processing as a monolithic procedure, adding subscription
orders would require rewriting the entire flow. The abstraction boundary
at <code>Order</code> allows the system to evolve incrementally.</p>
<hr />
<h2 id="the-type-system-spectrum-a-multi-dimensional-view">4. The Type
System Spectrum: A Multi-Dimensional View</h2>
<h3 id="beyond-the-simple-spectrum">4.1 Beyond the Simple Spectrum</h3>
<p>A common error is viewing type systems as a single spectrum from
“untyped” to “strongly typed.” As Cardelli and Wegner established in “On
Understanding Types, Data Abstraction, and Polymorphism” (1985), type
systems vary along multiple independent dimensions.</p>
<p><em>Static vs. dynamic</em> concerns when types are checked. Static
systems (Java, Haskell, Rust) catch errors before execution; dynamic
systems (Python, Ruby, JavaScript) catch them at the point of
failure.</p>
<p><em>Strong vs. weak</em> concerns how rigorously type rules are
enforced. Strong systems (Haskell, Python) generally forbid implicit
type coercion; weak systems (C, JavaScript) allow it. The distinction is
imprecise—Python permits some implicit numeric conversions
(<code>1 + 1.0 = 2.0</code>, <code>True + 1 = 2</code>) but rejects
others (<code>"a" + 1</code> raises TypeError). The key observation is
that static/dynamic and strong/weak are independent: Python is dynamic
but mostly strong, while C is static but weak.</p>
<p><em>Nominal vs. structural</em> concerns how compatibility is
determined. Nominal systems (Java, C#) require explicit type
declarations; structural systems (TypeScript interfaces) compare shapes.
Go occupies a middle ground: types are nominally declared but interfaces
are structurally satisfied (a type implements an interface if it has the
right methods, without explicit declaration).</p>
<p><em>First-order vs. higher-kinded</em> concerns what can be
parameterized. First-order polymorphism allows
<code>List&lt;T&gt;</code> where <code>T</code> is a type. Higher-kinded
polymorphism allows the container itself to be a parameter, so you can
write code that works generically with <code>List</code>,
<code>Set</code>, or <code>Option</code>. Haskell and Scala support
this; Java and Go do not.</p>
<p><em>Dependent types</em> allow types to depend on values. Most
languages keep types and values separate; Idris, Agda, and Coq blur this
boundary.</p>
<p>The ML family (Standard ML, OCaml, Haskell) deserves special mention.
“ML” here refers to the Meta Language tradition, not machine learning.
These languages combine static and strong typing with higher-kinded
polymorphism and Hindley-Milner type inference, which automatically
deduces types from usage without requiring explicit annotations.</p>
<h3 id="reynolds-parametricity-abstraction-through-types">4.2 Reynolds’
Parametricity: Abstraction Through Types</h3>
<p>John Reynolds’ “Types, Abstraction and Parametric Polymorphism”
(1983) and Wadler’s “Theorems for Free!” (1989) established that
parametric polymorphism provides abstraction guarantees.</p>
<p>The parametricity principle states that a parametrically polymorphic
function cannot inspect or depend on the specific type with which it is
instantiated. Because the function must work for <em>any</em> type, it
cannot call type-specific methods or compare values; it can only
rearrange, duplicate, or discard elements. From the type signature
alone, one can derive theorems about behavior.</p>
<p>For any function <code>f : ∀a. [a] → [a]</code> and any function
<code>g : A → B</code>, the following “free theorem” holds:</p>
<pre><code>map g . f_A = f_B . map g</code></pre>
<p>where <code>f_A</code> is <code>f</code> instantiated at type
<code>A</code> and <code>f_B</code> at type <code>B</code>. This
equation relates different type instantiations of the same polymorphic
function—a powerful result derived purely from the type signature.</p>
<p>In a purely parametric language (without runtime type inspection or
reflection), parametricity proves that: 1. <strong>Information hiding is
enforced</strong>: A function <code>∀a. a → a</code> must be the
identity 2. <strong>Representation independence</strong>: Parametric
clients work the same regardless of concrete representation 3.
<strong>Equational reasoning works</strong>: Program properties can be
proven from types alone</p>
<p><strong>Citation</strong>: Wadler, P. (1989). “Theorems for Free!”
FPCA ’89.</p>
<h3 id="static-typing-early-binding-for-safety">4.3 Static Typing: Early
Binding for Safety</h3>
<p>Static type systems bind type information at compile time. The
benefits are substantial:</p>
<p><strong>Compile-time error detection</strong>: Entire categories of
errors—type mismatches, null pointer dereferences, missing method
implementations—can be caught before deployment.</p>
<p><strong>Documentation through types</strong>: A function signature
like <code>findUser(userId: UserId): User?</code> communicates the
contract: this function takes a user ID, might return a user, might
return nothing. Types serve as documentation that cannot become
stale.</p>
<p><strong>Tooling support</strong>: Static types enable intelligent
autocomplete, safe refactoring, and reliable navigation.</p>
<p>In our order processing system, static typing catches errors early.
If <code>processPayment</code> expects a <code>PaymentMethod</code> but
receives an <code>Order</code>, the compiler rejects it immediately.
When the team later renames <code>Order.totalPrice</code> to
<code>Order.total</code>, the type checker identifies every call site
that must change. Without static types, these errors surface only in
production or through exhaustive testing.</p>
<h3 id="dynamic-typing-late-binding-for-flexibility">4.4 Dynamic Typing:
Late Binding for Flexibility</h3>
<p>Dynamic typing defers type decisions until runtime, providing genuine
advantages in certain contexts.</p>
<p><strong>Rapid prototyping</strong>: When exploring a problem domain,
type annotations can slow iteration. The REPL-driven workflow common in
Python, Ruby, and Lisp allows immediate feedback without declaring types
that may change as understanding develops.</p>
<p><strong>Metaprogramming and DSLs</strong>: Dynamic languages excel at
creating domain-specific languages and metaprogramming. Ruby on Rails,
for instance, uses dynamic method definition extensively to create
expressive APIs that would require significantly more boilerplate in
static languages.</p>
<p><strong>Heterogeneous data</strong>: When processing data from
external sources (JSON APIs, configuration files, user input), the
structure may not be known until runtime. Dynamic languages handle this
naturally; static languages require parsing into typed structures or
using escape hatches like <code>Any</code> types.</p>
<p><strong>Gradual typing</strong> represents a middle path: TypeScript,
Python’s type hints, and Typed Racket allow mixing typed and untyped
code. Teams can add types incrementally where they provide value.</p>
<p>Rich Hickey’s talk “Simple Made Easy” (2011) offers a complementary
perspective by distinguishing <em>simple</em> (unmixed, untangled) from
<em>easy</em> (familiar, close at hand). His alternative: instead of
objects with methods, use simple data structures (maps, arrays, sets)
with generic functions. This insight applies regardless of typing
discipline but aligns with the dynamic tradition’s preference for data
over types.</p>
<h3 id="what-does-empirical-research-show">4.5 What Does Empirical
Research Show?</h3>
<p>The most rigorous empirical studies come from Stefan Hanenberg and
colleagues:</p>
<p><strong>Hanenberg et al. (OOPSLA 2012)</strong>: “An Empirical Study
of the Influence of Static Type Systems on the Usability of Undocumented
Software” - Finding: Static types significantly helped developers when
documentation was absent - Static and dynamic typing showed similar
results for well-documented APIs</p>
<p><strong>Ray et al. (CACM 2017)</strong>: “A Large-Scale Study of
Programming Languages and Code Quality in GitHub” - Finding: Static
typing correlated with fewer bug-fix commits - Effect size: Small but
statistically significant - Major caveat: Correlation, not causation</p>
<p><strong>Gao et al. (ICSE 2017)</strong>: “To Type or Not to Type:
Quantifying Detectable Bugs in JavaScript” - Finding: ~15% of JavaScript
bugs could have been prevented by static typing</p>
<p><strong>Synthesis</strong>: The empirical evidence is weaker than
advocates on either side claim. Static types demonstrably assist API
discovery and documentation, catch certain bugs that other methods miss,
and correlate with fewer defects in large-scale studies. However, effect
sizes are modest, methodological challenges persist, and context
significantly influences outcomes.</p>
<hr />
<h2 id="abstraction-in-concurrent-systems">5. Abstraction in Concurrent
Systems</h2>
<p>Concurrency provides a particularly instructive domain for the
abstraction question because the stakes are high in both directions. The
state space of concurrent systems grows exponentially with interleaving
points, making abstraction essential for tractable reasoning. Yet
concurrency abstractions frequently leak, exposing programmers to the
very complexity they sought to avoid.</p>
<h3 id="process-algebras-formal-abstractions-for-concurrency">5.1
Process Algebras: Formal Abstractions for Concurrency</h3>
<p>Hoare’s CSP (Communicating Sequential Processes, 1978) abstracted
concurrent processes as mathematical entities communicating through
synchronized channels. Hoare proposed that input and output should be
treated as basic primitives, with parallel composition as a fundamental
structuring method. The key contribution was algebraic laws that enable
compositional reasoning: you can prove properties of complex systems by
proving properties of parts, then combining the proofs. This is
abstraction in Dijkstra’s sense—creating a semantic level where precise
reasoning becomes possible.</p>
<p>Milner’s CCS (Calculus of Communicating Systems, 1980) introduced
bisimulation equivalence: two processes are equivalent if no external
observer can distinguish them. This formalizes information hiding in
concurrent systems.</p>
<p>The π-calculus (Milner, Parrow, Walker, 1992) added mobility,
allowing channel names to be passed as data. Where CSP and CCS assumed
fixed communication topology, the π-calculus can express processes with
changing structure—a server that spawns new channels for each client, or
a network where connections form and dissolve dynamically. This
abstracts over communication topology itself, enabling formal reasoning
about the kind of dynamic reconfiguration common in modern distributed
systems.</p>
<h3 id="the-actor-model">5.2 The Actor Model</h3>
<p>Carl Hewitt, Peter Bishop, and Richard Steiger (1973) and Gul Agha
(1986) developed the Actor model, where autonomous entities process
messages from mailboxes. As Agha emphasized, the model provides a
framework for reasoning about concurrency that is simpler than
shared-variable approaches: each actor encapsulates both state and a
thread of control, eliminating data races by construction. Actors
abstract away threads, locks, shared memory, and location. An actor
reference works the same whether the actor runs locally or on a remote
machine. This location transparency is powerful but, as we will see,
also the source of abstraction leaks when network failures occur.</p>
<h3 id="why-concurrency-abstractions-leak">5.3 Why Concurrency
Abstractions Leak</h3>
<p>Edward Lee’s “The Problem with Threads” (2006) crystallized why
concurrency is so difficult to abstract. Threads, he argued, are “wildly
nondeterministic” as a computational model—the same code can produce
different interleavings on each run. Programmers must constrain this
nondeterminism through synchronization, but the constraints are not
enforced by the abstraction itself. Concurrency abstractions leak for
three fundamental reasons: timing is physical (no abstraction can fully
hide real-world latency and scheduling), failure modes multiply
(deadlocks, livelocks, and races are emergent properties that cannot be
detected compositionally), and performance is non-compositional (two
fast components may be slow when composed due to contention).</p>
<p>The evolution of concurrent programming reflects the
abstraction-commitment trade-off. The Actor model succeeds when
message-passing semantics suffice, hiding thread management and memory
synchronization behind a higher-level abstraction. Raw threads remain
necessary when performance demands precise control over scheduling and
memory layout. The choice depends on commitment level: actors for
flexibility, threads for optimization.</p>
<hr />
<h2 id="language-design-case-studies">6. Language Design Case
Studies</h2>
<h3 id="go-simplicity-with-abstraction-where-needed">6.1 Go: Simplicity
with Abstraction Where Needed</h3>
<p>Go, designed at Google by Griesemer, Pike, and Thompson, explicitly
prioritized simplicity. For its first decade (2012-2022), Go lacked
generics. The designers acknowledged that generics would make the
language more complicated, and they were unwilling to add complexity
without a design that preserved Go’s character.</p>
<p>This was not opposition to abstraction. Go provides interfaces for
behavioral polymorphism, and goroutines with channels for
concurrency—both significant abstractions. The resistance was
specifically to parametric polymorphism, which the designers felt would
complicate the language more than it would help typical Go programs.
When they finally added generics in version 1.18, the stated criterion
was that “Go still feels like Go.”</p>
<p>Go’s trade-off is explicit: accept some code repetition in exchange
for code that is easier to read, understand, and maintain by large
teams. This is a legitimate position, not a failure to understand
abstraction. The designers prioritized a different point on the
abstraction-concreteness spectrum than Haskell or Scala.</p>
<h3 id="java-the-erasure-compromise">6.2 Java: The Erasure
Compromise</h3>
<p>When generics were added in Java 5 (2004), designers chose type
erasure: generic parameters are checked at compile time but erased at
runtime. A <code>List&lt;String&gt;</code> and
<code>List&lt;Integer&gt;</code> become the same <code>List</code> at
runtime.</p>
<table>
<thead>
<tr>
<th>Erasure (Java)</th>
<th>Reification (C#)</th>
</tr>
</thead>
<tbody>
<tr>
<td>No runtime type info</td>
<td>Full runtime type info</td>
</tr>
<tr>
<td>No performance overhead</td>
<td>Memory/processing cost</td>
</tr>
<tr>
<td>Cannot do <code>new T()</code></td>
<td>Can instantiate type parameters</td>
</tr>
<tr>
<td>Backward compatible</td>
<td>Requires runtime support</td>
</tr>
</tbody>
</table>
<p>This is a leaky abstraction, yet it has been successful. Pragmatic
compromises work when leaks are well-documented and benefits
substantial.</p>
<h3 id="rust-safety-through-abstraction">6.3 Rust: Safety Through
Abstraction</h3>
<p>Rust uses sophisticated type-level abstractions to provide safety
guarantees without runtime cost. The ownership system ensures memory
safety at compile time:</p>
<p>Every value has a single owner. References are either shared (many
readers, no writers) or mutable (one writer, who can also read). These
rules are enforced through the type system, with no runtime overhead—the
checks happen entirely at compile time.</p>
<p>The “zero-cost abstraction” claim requires careful interpretation.
Ownership has zero <em>runtime</em> cost; the program runs as fast as
equivalent C. But Rust trades runtime cost for compile-time cost (longer
builds, more complex error messages) and learning-curve cost
(significant effort to master ownership). The abstraction is not free;
the costs are shifted to different phases of development. For systems
programming where runtime performance is critical, this trade-off is
often worthwhile. For rapid prototyping, the upfront costs may outweigh
the benefits.</p>
<hr />
<h2 id="the-cognitive-dimension">7. The Cognitive Dimension</h2>
<h3 id="why-abstraction-is-cognitively-hard">7.1 Why Abstraction Is
Cognitively Hard</h3>
<p>Human cognition evolved to handle concrete objects in physical space.
Abstraction requires reasoning without direct perception. A type
parameter <code>T</code> has no color, no shape. An interface defines
behavior without implementation; we cannot observe what it does.</p>
<p>Conceptual metaphor theory (Lakoff and Johnson, 1980) proposes that
much abstract thinking draws on mappings from embodied experience,
though the extent of this grounding remains debated. Programming
terminology often reflects physical metaphors: stacks that grow and
shrink, trees with branches and leaves, queues where items wait their
turn. Whether these names constitute deep conceptual structures or
convenient labels, they suggest that abstractions benefit from concrete
imagery.</p>
<h3 id="cognitive-load-and-working-memory">7.2 Cognitive Load and
Working Memory</h3>
<p>Modern cognitive science has refined our understanding of working
memory limits. Cowan (2001) argues the limit for focused attention is
approximately three to five chunks, revising Miller’s classic “seven
plus or minus two” estimate when rehearsal strategies are
controlled.</p>
<p>This is why abstraction matters: it is a chunking mechanism. A
well-designed function chunks a sequence of operations into a single
named concept.</p>
<p>Cognitive load theory, developed by John Sweller (1988),
distinguishes three types of load: <em>intrinsic</em> (inherent
complexity of the material), <em>extraneous</em> (complexity from how
information is presented), and <em>germane</em> (effort spent building
mental schemas, which is beneficial). Felienne Hermans applies this
framework to programming in “The Programmer’s Brain” (2021).</p>
<p>Effective abstraction reduces extraneous load by hiding irrelevant
detail. Yet abstraction can also <em>increase</em> cognitive load when
the abstraction itself must be understood. Each layer of indirection
adds context that must be maintained.</p>
<p>John Ousterhout, in “A Philosophy of Software Design” (2018),
distinguishes deep from shallow modules. A deep module has a simple
interface but powerful functionality (good abstraction). A shallow
module has a large interface for simple functionality (poor
abstraction).</p>
<h3 id="expert-vs.-novice">7.3 Expert vs. Novice</h3>
<p>Expertise changes what is cognitively tractable. Novices need
concrete examples because they have not yet internalized patterns. They
must reason step by step through code that experts grasp at a
glance.</p>
<p>Hailperin, Kaiser, and Knight capture this transformation in their
textbook “Concrete Abstractions” (1999), observing that with
familiarity, programmers begin to think of abstractions as “actual
concrete objects.” What begins as an abstract concept becomes through
practice a manipulable mental object. The expert experiences
abstractions as concrete things, not because they have become physical
but because they have become cognitively real. A senior developer “sees”
a monad or a futures chain as clearly as a novice sees an integer. This
is why experts sometimes underestimate the difficulty of their
abstractions: they have forgotten what it was like not to see them.</p>
<p>Educational research supports “concreteness fading”: start with
physical or visual representations, then gradually move to symbolic
abstraction (Goldstone &amp; Son, 2005; McNeil &amp; Fyfe, 2012).
Abstractions are learned through concrete examples, not despite
them.</p>
<hr />
<h2 id="the-balancing-act">8. The Balancing Act</h2>
<h3 id="context-dependent-trade-offs">8.1 Context-Dependent
Trade-offs</h3>
<p>The right balance depends on context:</p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>Favors Abstraction</th>
<th>Favors Concreteness</th>
</tr>
</thead>
<tbody>
<tr>
<td>Team size</td>
<td>Small, tight</td>
<td>Large, distributed</td>
</tr>
<tr>
<td>Domain knowledge</td>
<td>Uncertain, evolving</td>
<td>Well understood</td>
</tr>
<tr>
<td>Cost of bugs</td>
<td>High (safety-critical)</td>
<td>Low (fixable)</td>
</tr>
<tr>
<td>Interface commitment</td>
<td>Public API, protocol</td>
<td>Internal, refactorable</td>
</tr>
<tr>
<td>System lifespan</td>
<td>Long-lived</td>
<td>Prototype</td>
</tr>
</tbody>
</table>
<p><em>Rationale for team size:</em> Small teams can maintain shared
mental models that make sophisticated abstractions usable; members know
when to apply an abstraction and when to work around it. Large,
distributed teams lack this shared context—explicit, concrete code is
easier to understand without oral tradition. This does not mean large
teams should avoid abstraction, but that abstractions adopted by large
teams need better documentation, clearer contracts, and more explicit
examples.</p>
<h3 id="when-start-abstract-is-correct">8.2 When “Start Abstract” Is
Correct</h3>
<p>Starting abstract makes sense for safety-critical systems (where
certification requires formal specifications and failures are
catastrophic), protocol design (where multiple parties must implement
against a spec), API design (where the abstraction is a commitment and
concrete changes break compatibility), and mathematical libraries (where
algebraic properties must be preserved).</p>
<h3 id="reconciling-structure-late-with-start-concrete">8.3 Reconciling
“Structure Late” with “Start Concrete”</h3>
<p>Perlis advises “structure data late,” while practical wisdom often
says “start concrete.” These appear to conflict: if you start with
concrete data structures, have you not structured early?</p>
<p>The reconciliation lies in understanding what each opposes.
“Structure data late” opposes premature commitment to
<em>representation</em>—choosing fields, types, and relationships before
understanding the domain. “Start concrete” opposes premature
<em>abstraction</em>—creating interfaces and generic types before
knowing what varies.</p>
<p>These target different failure modes: - <strong>Premature
structuring</strong> creates data models that do not match reality,
forcing impedance mismatches throughout the system - <strong>Premature
abstraction</strong> creates interfaces that do not match actual
variation, adding indirection without benefit</p>
<p>The common principle is epistemic humility: defer decisions until you
have sufficient information. With data representation, learn the domain
before committing to schema. With abstraction, see actual variation
before generalizing. This is what Simon called “bounded
rationality”—making decisions appropriate to available information.</p>
<h3 id="the-refined-thesis">8.4 The Refined Thesis</h3>
<p>The formal methods community offers a legitimate counter-thesis:
“Start abstract, refine to concrete.” Stepwise refinement, pioneered by
Wirth (1971) and Dijkstra (1972), proves that starting with abstract
specifications and refining to implementations can guarantee correctness
by construction.</p>
<p>This approach is demonstrably superior for safety-critical systems,
protocols, and public APIs. A thesis that “concreteness should always be
the default” would be too narrow.</p>
<p>A more defensible position: <em>match abstraction level to commitment
level</em>. The appropriate level of abstraction is determined by the
cost and reversibility of decisions. Start concrete when exploration is
cheap and decisions are reversible. Start abstract when interfaces are
commitments, failures are costly, or formal properties must be
preserved.</p>
<p>Returning to our order processing system: the team should start
abstract for the public API that partners use to submit orders, because
changing it later breaks external integrations. They should start
concrete for internal pricing calculations, because the team can
refactor freely until they understand the domain. The payment provider
interface sits in between: if the contract with Stripe is firm, start
concrete; if switching providers is plausible, define a
<code>PaymentProvider</code> interface early. The decision depends not
on technical elegance but on commitment cost.</p>
<p>This framework provides clear criteria:</p>
<table>
<thead>
<tr>
<th>When to Start Concrete</th>
<th>When to Start Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td>Internal code, refactorable</td>
<td>Public APIs, protocols</td>
</tr>
<tr>
<td>Domain unclear, exploratory</td>
<td>Domain well-understood</td>
</tr>
<tr>
<td>Testing provides sufficient confidence</td>
<td>Formal proofs required</td>
</tr>
<tr>
<td>Single team, can coordinate</td>
<td>Multiple parties need contracts</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="limitations">9. Limitations</h2>
<p>This paper synthesizes perspectives on abstraction primarily from the
programming language and software design literature. Several important
dimensions remain outside its scope.</p>
<p><strong>Distributed systems</strong>: Network partitions, eventual
consistency, and the CAP theorem introduce abstraction challenges that
differ fundamentally from single-process programming. The “abstractions
leak” problem is especially severe when latency and failure are physical
realities that no interface can hide.</p>
<p><strong>Data layer</strong>: Database abstraction (SQL vs. NoSQL,
ORMs, query builders) involves distinct trade-offs not addressed here.
The impedance mismatch between object-oriented code and relational data
represents a well-studied abstraction problem with its own
literature.</p>
<p><strong>User interface</strong>: Component libraries, design systems,
and UI frameworks involve visual and interactive abstractions where
human perception and usability testing replace type checking as
validation mechanisms.</p>
<p><strong>Organizational factors</strong>: Conway’s Law suggests that
system structure mirrors organization structure. Abstraction decisions
are often constrained by team boundaries, communication patterns, and
political considerations that this paper does not address.</p>
<p><strong>Empirical limitations</strong>: The empirical evidence on
type systems (Section 4.5) is drawn from controlled studies and
repository mining, both of which have known limitations. Long-term
maintenance costs and developer satisfaction are difficult to measure
and remain under-studied.</p>
<p>The framework proposed here—matching abstraction level to commitment
level—is a heuristic, not a formula. It provides guidance for reasoning
about abstraction decisions but cannot replace domain knowledge and
engineering judgment.</p>
<hr />
<h2 id="conclusion-practical-guidance-for-software-engineers">10.
Conclusion: Practical Guidance for Software Engineers</h2>
<h3 id="five-principles-for-working-engineers">Five Principles for
Working Engineers</h3>
<p><strong>1. Delay abstraction until duplication becomes
costly.</strong></p>
<p>Avoid creating an interface for a single implementation. The first
implementation reveals what the abstraction should be. The second
confirms or refutes that understanding. The third is when extraction
becomes justified.</p>
<p><em>Concrete thresholds:</em> Consider abstraction when duplicated
code exceeds 15-20 lines, when you fix the same bug in multiple
locations, or when a requirements change forces edits in three or more
places. Note the pattern on the second occurrence; prioritize extraction
on the third.</p>
<p><strong>2. When you abstract, abstract as completely as
practical.</strong></p>
<p>A leaky abstraction can be worse than no abstraction because it
forces understanding both the abstraction and its implementation.
Pragmatic compromises can succeed when leaks are documented and benefits
substantial.</p>
<p><em>Completeness test:</em> Can clients use the abstraction without
knowing the implementation type? If code checks <code>instanceof</code>,
downcasts, or handles implementation-specific cases, the abstraction is
incomplete. Either fix the interface or acknowledge it as a leaky
convenience.</p>
<p><strong>3. Make your abstractions concrete through
examples.</strong></p>
<p>Every interface should have at least one example. Every generic type
should be instantiated somewhere visible. If you cannot provide a clear
example, you may not understand your own abstraction.</p>
<p><strong>4. Match abstraction level to commitment level.</strong></p>
<p>Abstract what is expensive to change (public APIs, protocols, safety
requirements). Leave concrete what is inexpensive to change (internal
code, exploratory work).</p>
<p><strong>5. Treat abstraction as a tool, not a virtue.</strong></p>
<p>The objective is working software that can be understood and
maintained. Sometimes three nearly-identical functions serve better than
one parameterized function. Abstraction carries costs (indirection,
cognitive load, learning curve) that must be weighed against
benefits.</p>
<p><em>Justification test:</em> Before adding abstraction, articulate
what specific problem it solves in one sentence. If you cannot explain
the benefit concisely, you probably do not need it yet.</p>
<h3 id="recognizing-abstraction-problems">Recognizing Abstraction
Problems</h3>
<p><em>Signs of over-abstraction</em>: You need to understand three or
more layers to trace a simple operation. Most interfaces have exactly
one implementation. New team members struggle to find where work
actually happens. Generic types proliferate:
<code>Handler&lt;Request&lt;Input&lt;T&gt;&gt;, Response&lt;Output&lt;U&gt;&gt;&gt;</code>.</p>
<p><em>Signs of under-abstraction</em>: Copy-paste is your primary reuse
mechanism. Bug fixes require changes in multiple similar locations. You
are afraid to refactor because you will miss a case. The same validation
logic appears in three or more places.</p>
<h3 id="abstraction-maintenance-and-lifecycle">Abstraction Maintenance
and Lifecycle</h3>
<p>Creating abstractions is the easy part. The harder challenge is
maintaining them as systems evolve.</p>
<p><strong>Abstractions drift from reality.</strong> The
<code>PaymentProvider</code> interface designed for Stripe may not fit
PayPal’s API. Teams add optional parameters, special-case methods, and
documented exceptions until the abstraction obscures more than it
reveals. When an abstraction requires understanding its violations to
use correctly, it has become technical debt.</p>
<p><strong>Abstractions accumulate.</strong> Teams rarely remove
abstractions; they add new ones alongside old. The result is
archaeological layers—each reasonable in isolation, collectively
incomprehensible. Good engineering practice includes abstraction
deprecation and removal, but this requires courage and coordination that
pressured teams often lack.</p>
<p><strong>Abstractions become load-bearing.</strong> Once fifty clients
depend on an interface, fixing design flaws becomes nearly impossible.
The abstraction that seemed provisional becomes permanent by accretion.
This argues for keeping abstractions private and provisional until their
design stabilizes.</p>
<p><strong>Signs an abstraction should be retired:</strong> Most
implementations work around it rather than through it. Documentation
focuses on exceptions rather than the rule. New team members are told
“ignore this interface, here’s what actually happens.”</p>
<h3 id="abstraction-in-brownfield-systems">Abstraction in Brownfield
Systems</h3>
<p>Most engineering happens in existing codebases where abstraction
boundaries are already established, often poorly. The question shifts
from “what abstraction is ideal?” to “where can I improve given current
constraints?”</p>
<p>Practical strategies include the <strong>strangler pattern</strong>
(wrap legacy code behind a new interface, gradually migrate),
<strong>anti-corruption layers</strong> (isolate legacy abstractions
from new code), and <strong>opportunistic refactoring</strong> (improve
abstractions incrementally during feature work). The commitment-level
framework still applies: invest in abstraction where the interface is
stable; tolerate concrete workarounds where change is ongoing.</p>
<h3 id="implications-for-ai-assisted-development">Implications for
AI-Assisted Development</h3>
<p>The emergence of AI coding assistants shifts the economics of the
abstraction decision without invalidating its logic. When AI can rapidly
generate concrete implementations and explore design alternatives, the
cost of “starting concrete” decreases substantially. Developers can
prototype multiple approaches before committing to an abstraction,
making the “Rule of Three” more practical to follow.</p>
<p>The framework presented here, matching abstraction level to
commitment level, becomes more important, not less. AI excels at
low-commitment work: generating implementations, suggesting patterns,
exploring alternatives. High-commitment decisions (defining public APIs,
choosing architectural boundaries, naming domain concepts) remain human
responsibilities. These decisions determine what the AI should generate
and how to evaluate its output.</p>
<p>This suggests a natural division: humans focus on abstraction design
and architectural judgment; AI assists with implementation and
exploration. The cognitive dimension remains critical: developers must
understand abstractions well enough to recognize whether AI-generated
code embodies them correctly. The art of appropriate abstraction shifts
from writing to evaluating, but the underlying skill persists: knowing
when and how much to abstract.</p>
<h3 id="the-shape-of-good-code">The Shape of Good Code</h3>
<p>Good code has a characteristic shape. At the boundaries, where
systems meet users, networks, and databases, it tends toward
concreteness. Types are specific. Validation is explicit. This is where
assumptions must be stated.</p>
<p>In the interior, where business logic lives, abstraction increases.
Common patterns are extracted. Domain concepts are named. This is where
essential complexity is managed.</p>
<h3 id="fred-brookss-essential-vs.-accidental-complexity">Fred Brooks’s
Essential vs. Accidental Complexity</h3>
<p>Fred Brooks distinguished essential complexity (inherent in the
problem) from accidental complexity (introduced by our tools).
Abstraction reduces accidental complexity by eliminating redundancy and
encapsulating implementation details. But abstraction cannot reduce
essential complexity—only hide it temporarily.</p>
<p>The purpose of abstraction is to eliminate accidental complexity
while <em>revealing</em> essential complexity clearly. A good
abstraction makes the hard parts visible and the easy parts invisible. A
poor abstraction does the reverse.</p>
<h3 id="final-synthesis">Final Synthesis</h3>
<p>The choice between abstraction and concreteness is not a problem to
be solved but a dimension to be navigated. The pioneers were not
abstractionists or concretists. Perlis valued late binding but warned
against premature simplification. Dijkstra valued abstraction but
demanded precision. Hoare wanted obvious correctness, achievable only
when abstraction reveals rather than hides. Simon explained why complex
systems require stable intermediate forms.</p>
<p>Hofstadter, in “Gödel, Escher, Bach” (1979), observed that mappings
between levels of description are often fragile, depending on tacit
assumptions that, if violated, destroy the mapping.</p>
<p>Practitioners create abstractions knowing they will leak, hide
complexity knowing it will surface, and separate concerns knowing they
will become entangled. This is not failure; it is the permanent
condition of building complex systems.</p>
<p>Match abstraction level to commitment level. Abstract where decisions
are expensive and irreversible. Stay concrete where exploration is cheap
and change is easy. When you abstract, do so with examples, with your
team’s expertise in mind, and with willingness to revisit as
requirements evolve.</p>
<p>This is not a formula but a discipline. It requires understanding
concrete cases before abstracting, honesty about whether abstractions
help, and willingness to refactor when they do not.</p>
<p>The need to calibrate abstraction is not a deficiency in programming
but an inherent characteristic of representation itself. Every
representation is a choice about what to emphasize and what to omit.
Good software engineering is the art of making those choices
deliberately, and revising them when circumstances change.</p>
<hr />
<h2 id="author-contributions-credit">Author Contributions (CRediT)</h2>
<p><strong>AI Contribution (Claude, Anthropic):</strong> Literature
review, source synthesis, initial drafting, structural organization.</p>
<p><strong>Human Contribution:</strong> Conceptualization, research
direction, source verification, critical review, editing, final
approval.</p>
<hr />
<h2 id="references">References</h2>
<h3 id="primary-sources">Primary Sources</h3>
<ol type="1">
<li>Perlis, A. (1982). “Epigrams on Programming.” ACM SIGPLAN Notices
17(9).</li>
<li>Dijkstra, E. W. (1972). “The Humble Programmer.” Communications of
the ACM 15(10). [EWD340]</li>
<li>Dijkstra, E. W. (1974). “On the Role of Scientific Thought.”
[EWD447]</li>
<li>Hoare, C. A. R. (1981). “The Emperor’s Old Clothes.” Communications
of the ACM 24(2).</li>
<li>McCarthy, J. (1960). “Recursive Functions of Symbolic Expressions
and Their Computation by Machine, Part I.” Communications of the ACM
3(4).</li>
<li>Liskov, B. &amp; Zilles, S. (1974). “Programming with Abstract Data
Types.” ACM SIGPLAN Notices 9(4).</li>
<li>Liskov, B. &amp; Wing, J. (1994). “A Behavioral Notion of
Subtyping.” ACM TOPLAS 16(6).</li>
<li>Parnas, D. L. (1972). “On the Criteria To Be Used in Decomposing
Systems into Modules.” Communications of the ACM 15(12).</li>
<li>Simon, H. A. (1969, 1996). The Sciences of the Artificial, 3rd
ed. MIT Press.</li>
<li>Church, A. (1936). “An Unsolvable Problem of Elementary Number
Theory.” American Journal of Mathematics 58(2).</li>
</ol>
<h3 id="type-theory-and-formal-foundations">Type Theory and Formal
Foundations</h3>
<ol start="11" type="1">
<li>Cardelli, L. &amp; Wegner, P. (1985). “On Understanding Types, Data
Abstraction, and Polymorphism.” Computing Surveys 17(4).</li>
<li>Reynolds, J. C. (1983). “Types, Abstraction and Parametric
Polymorphism.” Information Processing 83.</li>
<li>Wadler, P. (1989). “Theorems for Free!” FPCA ’89.</li>
<li>Moggi, E. (1991). “Notions of Computation and Monads.” Information
and Computation 93(1).</li>
<li>Wadler, P. (1995). “Monads for Functional Programming.” Advanced
Functional Programming.</li>
<li>Cousot, P. &amp; Cousot, R. (1977). “Abstract Interpretation: A
Unified Lattice Model for Static Analysis of Programs.” POPL ’77.</li>
<li>Pierce, B. C. (2002). Types and Programming Languages. MIT
Press.</li>
</ol>
<h3 id="concurrency">Concurrency</h3>
<ol start="18" type="1">
<li>Hoare, C. A. R. (1978). “Communicating Sequential Processes.”
Communications of the ACM 21(8).</li>
<li>Milner, R. (1980). A Calculus of Communicating Systems. Springer
LNCS 92.</li>
<li>Milner, R., Parrow, J. &amp; Walker, D. (1992). “A Calculus of
Mobile Processes.” Information and Computation 100(1).</li>
<li>Hewitt, C., Bishop, P. &amp; Steiger, R. (1973). “A Universal
Modular ACTOR Formalism for Artificial Intelligence.” IJCAI ’73.</li>
<li>Agha, G. (1986). Actors: A Model of Concurrent Computation in
Distributed Systems. MIT Press.</li>
<li>Lee, E. A. (2006). “The Problem with Threads.” IEEE Computer
39(5).</li>
</ol>
<h3 id="software-architecture">Software Architecture</h3>
<ol start="24" type="1">
<li>Garlan, D. &amp; Shaw, M. (1994). “An Introduction to Software
Architecture.” CMU-CS-94-166.</li>
<li>Bass, L., Clements, P. &amp; Kazman, R. (2021). Software
Architecture in Practice, 4th ed. Addison-Wesley.</li>
</ol>
<h3 id="empirical-studies">Empirical Studies</h3>
<ol start="26" type="1">
<li>Hanenberg, S. et al. (2012). “An Empirical Study of the Influence of
Static Type Systems on the Usability of Undocumented Software.” OOPSLA
’12.</li>
<li>Ray, B. et al. (2017). “A Large-Scale Study of Programming Languages
and Code Quality in GitHub.” Communications of the ACM.</li>
<li>Gao, Z. et al. (2017). “To Type or Not to Type: Quantifying
Detectable Bugs in JavaScript.” ICSE ’17.</li>
</ol>
<h3 id="language-documentation">Language Documentation</h3>
<ol start="29" type="1">
<li>The Go Blog. “Why Generics?”</li>
<li>Oracle. “Java Generics Tutorial.”</li>
<li>The Rust Programming Language. “Understanding Ownership.”</li>
</ol>
<h3 id="talks-and-online-sources">Talks and Online Sources</h3>
<ol start="32" type="1">
<li>Hickey, R. (2011). “Simple Made Easy.” Strange Loop Conference.</li>
</ol>
<h3 id="books">Books</h3>
<ol start="33" type="1">
<li>Hermans, F. (2021). The Programmer’s Brain. Manning
Publications.</li>
<li>Ousterhout, J. (2018). A Philosophy of Software Design. Yaknyam
Press.</li>
<li>Lakoff, G. &amp; Johnson, M. (1980). Metaphors We Live By.
University of Chicago Press.</li>
<li>Hailperin, M., Kaiser, B. &amp; Knight, K. (1999). Concrete
Abstractions. Brooks/Cole Publishing.</li>
<li>Hofstadter, D. R. (1979). Gödel, Escher, Bach: An Eternal Golden
Braid. Basic Books.</li>
<li>Brooks, F. P. (1987). “No Silver Bullet: Essence and Accidents of
Software Engineering.” Computer 20(4).</li>
<li>Spolsky, J. (2002). “The Law of Leaky Abstractions.”</li>
</ol>
<h3 id="academic-papers">Academic Papers</h3>
<ol start="40" type="1">
<li>Wirth, N. (1971). “Program Development by Stepwise Refinement.”
Communications of the ACM 14(4).</li>
<li>Cowan, N. (2001). “The Magical Number 4 in Short-term Memory.”
Behavioral and Brain Sciences 24(1).</li>
<li>Goldstone, R. L. &amp; Son, J. Y. (2005). “The Transfer of
Scientific Principles Using Concrete and Idealized Simulations.” Journal
of the Learning Sciences 14(1).</li>
<li>McNeil, N. M. &amp; Fyfe, E. R. (2012). “‘Concreteness Fading’
Promotes Transfer of Mathematical Knowledge.” Learning and Instruction
22(6).</li>
<li>Sweller, J. (1988). “Cognitive Load During Problem Solving.”
Cognitive Science 12(2).</li>
</ol>
</body>
</html>
